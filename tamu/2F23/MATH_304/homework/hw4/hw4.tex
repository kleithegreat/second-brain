\documentclass{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\lhead{4th Homework - MATH 304 508}
\rhead{Kevin Lei}
\renewcommand{\headrulewidth}{0.4pt}

\begin{document}

\section*{Question 1}
Determine whether the following subsets are subspaces:

\subsection*{Part a}
$S_{1} := \{(x_{1},x_{2})^{T} \in \mathbb{R}^2 : x_{1} = \sqrt{123}x_{2}\}$

\textbf{Answer:} This is a subspace of $\mathbb{R}^2$ since it is a straight line that passes through the origin.

\begin{proof}
Let $(a,b)$ and $(c,d)$ be two elements of $S_{1}$.
We want to show that $(a,b) + (c,d) \in S_{1}$ and $n(a,b) \in S_{1} \forall n \in \mathbb{R}$.
Using the definition of $S_{1}$, we have that $a=\sqrt{123}b$ and $c=\sqrt{123}d$.
Adding elements $(a,b)$ and $(c,d)$, we have that $(a,b) + (c,d) = (a+c,b+d)$
We can then substitute in the values of $a$ and $c$ to get $(a+c,b+d) = (\sqrt{123}b+\sqrt{123}d,b+d)$
This can then be factored to $(\sqrt{123}(b+d),b+d)$
Since this satisfies the definition of $S_{1}$, we have that $(a,b) + (c,d) \in S_{1}$.
To show that $n(a,b) \in S_{1} \forall n \in \mathbb{R}$, we can use the definition of $S_{1}$ again.
The element $(a,b)$ can be written as $(\sqrt{123}b,b)$.
Multiplying this by $n$ gives us $(n\sqrt{123}b,nb)$.
Since this satisfies the definition of $S_{1}$, we have that $n(a,b) \in S_{1} \forall n \in \mathbb{R}$.
\end{proof}

\subsection*{Part b}
$S_{2} := \{(x_{1},x_{2})^{T} \in \mathbb{R}^2 : x_{1}x_{2} = 1\}$

\textbf{Answer:} This is not a subspace of $\mathbb{R}^2$ since it does not satisfy the addition property.

\begin{proof}
Let $(a,b)$ and $(c,d)$ be two elements of $S_{2}$.
Seeking a contradiction, lets assume that $(a,b) + (c,d) \in S_{2}$.
Since we can write $(a,b) + (c,d)$ as $(a+c,b+d)$, our assumption would imply that $(a+c)(b+d) = 1$.
Expanding this, we get $ab+ad+bc+cd = 1$.
It is given that $ab=1$ and $cd=1$, so we can substitute these in to get $1+ad+bc+1 = 1$.
This can be simplified to $ad+bc = -1$.
Since $a$ can be rewritten as $\frac{1}{b}$ and $c$ can be rewritten as $\frac{1}{d}$, we can substitute these in to get $\frac{d}{b}+\frac{b}{d} = -1$.
Multiplying the entire equation by $bd$ gives us $d^2+b^2 = -bd$.
Since $d^2$ and $b^2$ must be positive, this equation cannot be true, and thus we have reached a contradiction.
Therefore, $(a,b) + (c,d) \notin S_{2}$, so $S_{2}$ is not a subspace of $\mathbb{R}^2$.
\end{proof}

\subsection*{Part c}
$S_{3} := \{\text{the set of singular } 2 \times 2 \text{ matrices}\}$

\textbf{Answer:} This is not a subspace of $\mathbb{R}^{2 \times 2}$ since it does not satisfy the addition property. % How do I prove rigorously?

\textbf{Counterexample:} Let matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $B = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$ where A, B $\in S_{3}$.
$A + B = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, which is not a singular matrix.

\subsection*{Part d}
Let \( A \) be a fixed (but arbitrary) \( 2 \times 2 \) matrix.
\newline
$S_{4} := \{B \in \mathbb{R}^{2 \times 2} : BA = 0\}$

\textbf{Answer:} This is a subspace of $\mathbb{R}^{2 \times 2}$ .

\begin{proof}
Let $J$ and $K$ be two arbitrary elements in $S_{4}$.
We want to show that $J$, $K$ $\in S_{4} \rightarrow J + K \in S_{4}$ and $aJ \in S_{4} \forall a \in \mathbb{R}$.
In order for $J + K$ to be in $S_{4}$, we must have that $(J+K)A = 0$.
We can rewrite this as $JA + KA = 0$.
Since $J$ and $K$ are in $S_{4}$, we know that $JA = 0$ and $KA = 0$.
Substituting these in, we get $0 + 0 = 0$, which is true.
Therefore, $J + K \in S_{4}$.
Additionally, in order for $aJ$ to be in $S_{4}$, we must show that $\forall a \in \mathbb{R}, J \in S_{4} \rightarrow aJ \in S_{4}$.
To do this, we must verify the validity of $(aJ)A = 0$.
Since matrix multiplication is associative, we can rewrite this as $a(JA) = 0$.
Since $J$ is in $S_{4}$, we know that $JA = 0$.
Substituting this in, we get $a(0) = 0$, which is true.
Therefore, $aJ \in S_{4}$, so $S_{4}$ is a subspace of $\mathbb{R}^{2 \times 2}$.
\end{proof}

\subsection*{Part e}
$S_{5} := \{\text{the set of all polynomials of degree 2 or 4}\}$

\textbf{Answer:} This is not a subspace of $\mathbb{P}_{n}$. % Pn or P4? How prove rigorously?

\textbf{Counterexample:}


Let $p(x), q(x) \in S_{5}$.
Suppose that $p(x) = x^2$ and $q(x) = -x^2$.


Then $p(x) + q(x) = 0$, which is not a polynomial of degree 2 or 4.

\subsection*{Part f}
$S_{6} := \{\text{the set of upper triangular } 2 \times 2 \text{ matrices}\}$

\textbf{Answer:} This is a subspace of $\mathbb{R}^{2 \times 2}$.

\begin{proof}
Let \( A \) and \( B \) be two arbitrary elements in \( S_{6} \).
To prove that \( S_{6} \) is a subspace of \( \mathbb{R}^{2 \times 2} \), we must show that \( A \), \( B \) \( \in S_{6} \rightarrow A + B \in S_{6} \) and \( \forall n \in \mathbb{R}, nA \in S_{6} \).
Given that \( A \) and \( B \) are in \( S_{6} \), we know that \( A \) and \( B \) are upper triangular matrices.
Writing these out, we have:
\begin{align*} 
    A &= \begin{bmatrix} a_{11} & a_{12} \\ 0 & a_{22} \end{bmatrix} \\ 
    B &= \begin{bmatrix} b_{11} & b_{12} \\ 0 & b_{22} \end{bmatrix} 
\end{align*}
Since matrix addition is element-wise, we can write \( A + B \) as:
\begin{align*} 
    A + B &= \begin{bmatrix} a_{11} + b_{11} & a_{12} + b_{12} \\ 0 & a_{22} + b_{22} \end{bmatrix}
\end{align*}
It is clear in this form that \( A + B \) is an upper triangular matrix, so \( A + B \in S_{6} \).

To show that \( \forall n \in \mathbb{R}, nA \in S_{6} \), we must show that \( nA \) is an upper triangular matrix.
We know that \( nA \) is an upper triangular matrix if \( nA \) is of the form:
\begin{align*} 
    nA &= \begin{bmatrix} a_{11} & a_{12} \\ 0 & a_{22} \end{bmatrix}
\end{align*}
Since scalar multiplication is distributed element-wise in matricies, we can write \( nA \) as:
\begin{align*} 
    nA &= \begin{bmatrix} na_{11} & na_{12} \\ 0 & na_{22} \end{bmatrix}
\end{align*}
In this form, it is clear that \( nA \) is an upper triangular matrix.
Therefore, \( nA \in S_{6} \), so \( S_{6} \) is a subspace of \( \mathbb{R}^{2 \times 2} \).
\end{proof}
    
\subsection*{Part g}
$S_{7} := \{p \in \mathbb{P}_{4} : p(0) = 0\}$

\textbf{Answer:} This is a subspace of $\mathbb{P}_{4}$.

\begin{proof}
Let $f(x)$ and $g(x)$ be two arbitrary elements in $S_{7}$.
To prove that $S_{7}$ is a subspace of $\mathbb{P}_{4}$, we must show that $f(x)$, $g(x)$ $\in S_{7} \rightarrow f(x) + g(x) \in S_{7}$ and $\forall n \in \mathbb{R}, nf(x) \in S_{7}$.
Given that $f(x)$ and $g(x)$ are in $S_{7}$, we know that $f(0) = 0$ and $g(0) = 0$, which is by definition of $S_{7}$.
This means that $f(x)$ and $g(x)$ are of the form:
\begin{align*} 
    f(x) &= a_{1}x^4 + a_{2}x^3 + a_{3}x^2 + a_{4}x + 0 \\ 
    g(x) &= b_{1}x^4 + b_{2}x^3 + b_{3}x^2 + b_{4}x + 0
\end{align*}
This means we can write $f(x) + g(x)$ as:
\begin{align*} 
    f(x) + g(x) &= (a_{1} + b_{1})x^4 + (a_{2} + b_{2})x^3 + (a_{3} + b_{3})x^2 + (a_{4} + b_{4})x + 0
\end{align*}
In this form it is clear that $f(x) +g(x)$ satisfies the condition that $p(0) = 0$, so $f(x) + g(x) \in S_{7}$.

To show that $\forall n \in \mathbb{R}, nf(x) \in S_{7}$, we can follow a similar process.
We know that $nf(x)$ is of the form:
\begin{align*} 
    nf(x) &= na_{1}x^4 + na_{2}x^3 + na_{3}x^2 + na_{4}x + 0
\end{align*}
Regardless of the value of $n$, $nf(x)$ will always satisfy the condition that $p(0) = 0$, so $nf(x) \in S_{7}$.
Therefore, $S_{7}$ is a subspace of $\mathbb{P}_{4}$.
\end{proof}

\newpage
\subsection*{Question 2}
Find the null space of the following matrices:
\begin{align*}
A = \begin{bmatrix} 2 & 1 & 0 \\ 4 & -1 & 1 \end{bmatrix}, \quad
B = \begin{bmatrix} -1 & -2 & 2 & 1 \\ 2 & 4 & -4 & -2 \end{bmatrix}, \quad
C = \begin{bmatrix} 0 & 1 & 4 \\ 1 & 0 & 3 \\ 4 & 3 & 0 \end{bmatrix}
\end{align*}

\begin{align*}
&\textbf{Matrix A:}
\\
&\begin{bmatrix} 2 & 1 & 0 \\ 4 & -1 & 1 \end{bmatrix} \sim 
\begin{bmatrix} 2 & 1 & 0 \\ 0 & -3 & 1 \end{bmatrix} \sim 
\begin{bmatrix} 2 & 1 & 0 \\ 0 & 1 & -\frac{1}{3} \end{bmatrix} \sim
\begin{bmatrix} 2 & 0 & \frac{1}{3} \\ 0 & 1 & -\frac{1}{3} \end{bmatrix} \sim
\begin{bmatrix} 1 & 0 & \frac{1}{6} \\ 0 & 1 & -\frac{1}{3} \end{bmatrix}
\\
&\begin{cases} x_{1} + \frac{1}{6}x_{3} = 0 \\ x_{2} - \frac{1}{3}x_{3} = 0 \end{cases} \rightarrow 
\begin{cases} x_{1} = -\frac{1}{6}x_{3} \\ x_{2} = \frac{1}{3}x_{3} \end{cases} \rightarrow 
\begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \end{bmatrix} = 
\begin{bmatrix} -\frac{1}{6}x_{3} \\ \frac{1}{3}x_{3} \\ x_{3} \end{bmatrix} = 
x_{3}\begin{bmatrix} -\frac{1}{6} \\ \frac{1}{3} \\ 1 \end{bmatrix}
\\
&N(A) = \left\{a\begin{bmatrix} -\frac{1}{6} \\ \frac{1}{3} \\ 1 \end{bmatrix} : a \in \mathbb{R}\right\} = 
\text{span}\left\{\begin{bmatrix} -\frac{1}{6} \\ \frac{1}{3} \\ 1 \end{bmatrix}\right\}
\\
&\textbf{Matrix B:}
\\
&\begin{bmatrix} -1 & -2 & 2 & 1 \\ 2 & 4 & -4 & -2 \end{bmatrix} \sim
\begin{bmatrix} -1 & -2 & 2 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} \sim
-x_{1} - 2x_{2} + 2x_{3} + x_{4} = 0
\\
&\begin{cases} x_{1} = -2x_{2} + 2x_{3} + x_{4} \\ x_{2} = x_{2} \\ x_{3} = x_{3} \\ x_{4} = x_{4} \end{cases} \rightarrow
\begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \end{bmatrix} =
\begin{bmatrix} -2x_{2} + 2x_{3} + x_{4} \\ x_{2} \\ x_{3} \\ x_{4} \end{bmatrix}
=
x_{2}\begin{bmatrix} -2 \\ 1 \\ 0 \\ 0 \end{bmatrix} + x_{3}\begin{bmatrix} 2 \\ 0 \\ 1 \\ 0 \end{bmatrix} + x_{4}\begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}
\\
&N(B) = \left\{a\begin{bmatrix} -2 \\ 1 \\ 0 \\ 0 \end{bmatrix} + b\begin{bmatrix} 2 \\ 0 \\ 1 \\ 0 \end{bmatrix} + c\begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix} : a,b,c \in \mathbb{R}\right\} =
\text{span}\left\{\begin{bmatrix} -2 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}\right\}
\\
&\textbf{Matrix C:}
\\
&\begin{bmatrix} 0 & 1 & 4 \\ 1 & 0 & 3 \\ 4 & 3 & 0 \end{bmatrix} \sim
\begin{bmatrix} 1 & 0 & 3 \\ 0 & 1 & 4 \\ 4 & 3 & 0 \end{bmatrix} \sim
\begin{bmatrix} 1 & 0 & 3 \\ 0 & 1 & 4 \\ 0 & 3 & -12 \end{bmatrix} \sim
\begin{bmatrix} 1 & 0 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & -24 \end{bmatrix}
\\
&\begin{cases} x_{1} + 3x_{3} = 0 \\ x_{2} + 4x_{3} = 0 \\ -24x_{3} = 0 \end{cases} \rightarrow
x_{1} = x_{2} = x_{3} = 0
\\
&N(C) = \left\{\begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}\right\}
\end{align*}

\newpage
\subsection*{Question 3}
Show that the following matrices form a spanning set for \(\mathbb{R}^{2 \times 2}\). Also, show that these matrices are linearly independent.
\begin{align*}
A_{11} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}, \quad
A_{12} = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}, \quad
A_{21} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}, \quad
A_{22} = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
\end{align*}

\noindent
\textbf{Proposition:} The matricies $A_{11}$, $A_{12}$, $A_{21}$, and $A_{22}$ form a spanning set for $\mathbb{R}^{2 \times 2}$

\begin{proof}
Suppose we have some arbitrary $A$ in $\mathbb{R}^{2 \times 2}$.
To show that the matricies $A_{11}$, $A_{12}$, $A_{21}$, and $A_{22}$ form a spanning set for $\mathbb{R}^{2 \times 2}$, 
we must show that $A$ can be written as a linear combination of $A_{11}$, $A_{12}$, $A_{21}$, and $A_{22}$.
We can write $A$ as:
\begin{align*}
A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
\end{align*}
Which means we need to find scalars $x_{1}$, $x_{2}$, $x_{3}$, and $x_{4}$ such that:
\begin{align*}
\begin{bmatrix} a & b \\ c & d \end{bmatrix} = 
x_{1}\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} + 
x_{2}\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} + 
x_{3}\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} + 
x_{4}\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
\end{align*}
From here we can see that
\begin{align*}
\begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} x_{1} & x_{2} \\ x_{3} & x_{4} \end{bmatrix}
\end{align*}
Therefore, $x_{1} = a$, $x_{2} = b$, $x_{3} = c$, and $x_{4} = d$.
Since we can write $A$ as a linear combination of $A_{11}$, $A_{12}$, $A_{21}$, and $A_{22}$, 
we have proven that the matricies $A_{11}$, $A_{12}$, $A_{21}$, and $A_{22}$ form a spanning set for $\mathbb{R}^{2 \times 2}$.
\end{proof}

\noindent
\textbf{Proposition:} The matricies $A_{11}$, $A_{12}$, $A_{21}$, and $A_{22}$ are linearly independent.

\begin{proof}
A set of vectors is linearly independent if and only if 
the only solution to $c_{1}v_{1} + c_{2}v_{2} + \cdots + c_{n}v_{n} = 0$ is $c_{1} = c_{2} = \cdots = c_{n} = 0$.
This means we have:
\begin{align*}
c_{1}\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} +
c_{2}\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} +
c_{3}\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} +
c_{4}\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} =
\begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}
\end{align*}
From this we have the following system:
\begin{align*}
\begin{cases} c_{1} \times 1 = 0 \\ c_{2} \times 1 = 0 \\ c_{3} \times 1 = 0 \\ c_{4} \times 1 = 0 \end{cases}
\end{align*}
From this it is clear that the only solution to the system is $c_{1} = c_{2} = c_{3} = c_{4} = 0$.
This is exactly the definition of linear independence, so we have proven that the matricies $A_{11}$, $A_{12}$, $A_{21}$, and $A_{22}$ are linearly independent.
\end{proof}

\newpage
\subsection*{Question 4}
Let \(x_{1}\), \(x_{2}\), and \(x_{3}\) be linearly independent vectors in \(\mathbb{R}^{n}\). Let
\begin{align*}
y_{1} = x_{1} + x_{2}, \quad y_{2} = x_{2} + x_{3}, \quad y_{3} = x_{3} + x_{1}.
\end{align*}
Decide if \(y_{1}\), \(y_{2}\), and \(y_{3}\) are linearly independent or not.

\noindent
\textbf{Answer:} The vectors $y_{1}$, $y_{2}$, and $y_{3}$ are linearly independent.

\begin{proof}
A set of vectors \{$v_{1}$, $v_{2}$, \ldots, $v_{n}$\} is linearly independent if and only if
the only solution to $c_{1}v_{1} + c_{2}v_{2} + \cdots + c_{n}v_{n} = 0$ is $c_{1} = c_{2} = \cdots = c_{n} = 0$.
This means we have the following equation:
$$ c_{1}y_{1} + c_{2}y_{2} + c_{3}y_{3} = 0 $$
For the set of vectors \{$y_{1}$, $y_{2}$, $y_{3}$\} to be linearly independent, 
we must show that the only solution to this equation is $c_{1} = c_{2} = c_{3} = 0$.
Using the definitions of $y_{1}$, $y_{2}$, and $y_{3}$, we can rewrite this equation as:
$$ c_{1}(x_{1} + x_{2}) + c_{2}(x_{2} + x_{3}) + c_{3}(x_{3} + x_{1}) = 0 $$
Distributing the coefficients, we get:
$$ c_{1}x_{1} + c_{1}x_{2} + c_{2}x_{2} + c_{2}x_{3} + c_{3}x_{3} + c_{3}x_{1} = 0 $$
Factoring out the $x_{i}$ terms, we get:
$$ (c_{1} + c_{3})x_{1} + (c_{1} + c_{2})x_{2} + (c_{2} + c_{3})x_{3} = 0 $$
Since it is given that $x_{1}$, $x_{2}$, and $x_{3}$ are linearly independent,
the only way for this equation to equal zero is if $c_{1} + c_{3} = 0$, $c_{1} + c_{2} = 0$, and $c_{2} + c_{3} = 0$.
This means we have the following system:
$$ \begin{cases} c_{1} + c_{3} = 0 \\ c_{1} + c_{2} = 0 \\ c_{2} + c_{3} = 0 \end{cases} $$
To solve this system we can use an augmented matrix.
$$\left[\begin{array}{ccc|c}
1 & 0 & 1 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 1 & 1 & 0 \\ \end{array}\right]$$
Using row operations, we get the following equivalent matricies:
$$\left[\begin{array}{ccc|c}
1 & 0 & 1 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 1 & 1 & 0 \\ \end{array}\right] \sim
\left[\begin{array}{ccc|c}
1 & 0 & 1 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 2 & 0 \\ \end{array}\right] \sim
\left[\begin{array}{ccc|c}
1 & 0 & 1 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 1 & 0 \\ \end{array}\right] \sim
\left[\begin{array}{ccc|c}
1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ \end{array}\right]$$
From this we can see that the only solution to the system is $c_{1} = c_{2} = c_{3} = 0$.
Since this is the only solution to the system, we have proven that the vectors $y_{1}$, $y_{2}$, and $y_{3}$ are linearly independent.
\end{proof}

\end{document}
